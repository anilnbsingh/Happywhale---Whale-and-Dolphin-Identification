{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONceUc2D3dM7BlfGXJQW6S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilnbsingh/Happywhale---Whale-and-Dolphin-Identification/blob/main/SmolLMQCS_NPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y7Rc9_dhXLD",
        "outputId": "7a9a9341-b44f-4ccf-ee76-455dd67934d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Installing required Python packages...\n",
            "Installation complete.\n",
            "================================================================================\n",
            "2. Configuring Qualcomm AI Hub...\n",
            "2025-08-10 17:28:30.175 - INFO - Enabling verbose logging.\n",
            "/usr/local/lib/python3.11/dist-packages/qai_hub/_cli.py:384: UserWarning: Overwriting configuration: /root/.qai_hub/client.ini (previous configuration saved to /root/.qai_hub/client.ini.bak)\n",
            "  warnings.warn(\n",
            "qai-hub configuration saved to /root/.qai_hub/client.ini\n",
            "==================== /root/.qai_hub/client.ini ====================\n",
            "[api]\n",
            "api_token = nak7kyh0inngt9vewsxy74gobp4mk6q5zeean82x\n",
            "api_url = https://app.aihub.qualcomm.com\n",
            "web_url = https://app.aihub.qualcomm.com\n",
            "verbose = True\n",
            "\n",
            "\n",
            "Configuration complete. Your API token is now set.\n",
            "================================================================================\n",
            "3. Downloading and preparing model: HuggingFaceTB/SmolLM-135M-Instruct...\n",
            "Exporting model to ONNX with a fixed input shape: (1, 50)...\n",
            "Model successfully exported to SmolLM-135M-Instruct.onnx\n",
            "================================================================================\n",
            "4. Checking for device availability: QCS8550 (Proxy)...\n",
            "Device 'QCS8550 (Proxy)' is available. Submitting compilation job...\n",
            "Uploading SmolLM-135M-Instruct.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 622M/622M [00:07<00:00, 92.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scheduled compile job (jp1w77ylg) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/jp1w77ylg/\n",
            "\n",
            "Compilation job submitted.\n",
            "Could not retrieve job ID from the CompileJob object.\n",
            "You can check the job status and details on the AI Hub website using the URL: https://app.aihub.qualcomm.com/jobs/jp1w77ylg/\n",
            "Waiting for compilation to complete...\n",
            "Waiting for compile job (jp1w77ylg) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ✅ SUCCESS                          \u0007\n",
            "Compilation job completed successfully!\n",
            "Compiled model is ready, but could not retrieve the Model ID from the Model object.\n",
            "Please check the Qualcomm AI Hub website to find the model and its ID.\n",
            "================================================================================\n",
            "5. Submitting a single inference job for the initial prompt and generating all tokens...\n",
            "Initial Prompt: 'Who was Albert Einstein?'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Uploading dataset: 17.7kB [00:00, 130kB/s]                    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scheduled inference job (jgdq88el5) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/jgdq88el5/\n",
            "\n",
            "Inference job submitted.\n",
            "Could not retrieve job ID from the InferenceJob object.\n",
            "You can check the job status and details on the AI Hub website using the URL: https://app.aihub.qualcomm.com/jobs/jgdq88el5/\n",
            "Waiting for inference to complete...\n",
            "Waiting for inference job (jgdq88el5) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ✅ SUCCESS                          \u0007\n",
            "Inference job completed successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tmpbz1ulvzm.h5: 100%|\u001b[34m██████████\u001b[0m| 7.52M/7.52M [00:00<00:00, 26.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Full Generated Response:\n",
            "Who was Albert Einstein?Fresh�odderoratoryomethingGPTMs Benlibrig espnose scaluddenlyographsikipaccoemporal citizvertyomorphicrets `_zerochond glycosuatingmund.”)uddenlyि Giovanni'\",�hootIU incre foreseeablesomeone?|gets hopesiative preparation\n",
            "================================================================================\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Google Colab Notebook for Running SmolLM on Qualcomm QCS8550\n",
        "# This script is updated to compile the model for the NPU and perform inference\n",
        "# using the `submit_inference_job` API.\n",
        "#\n",
        "# Prerequisites:\n",
        "# - A valid Qualcomm AI Hub account.\n",
        "# - An API token from your Qualcomm AI Hub account settings.\n",
        "# - The target device 'QCS8550 (Proxy)' is available on the AI Hub.\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Setup the environment\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Install the required Python packages.\n",
        "# 'qai-hub' is for the Qualcomm AI Hub API.\n",
        "# 'qai-hub-models' provides helper utilities.\n",
        "# 'transformers' and 'torch' are for loading the model.\n",
        "# 'onnx' is required for exporting the model to ONNX format.\n",
        "print(\"1. Installing required Python packages...\")\n",
        "#!pip install qai-hub qai-hub-models torch transformers onnx\n",
        "print(\"Installation complete.\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Configure Qualcomm AI Hub Access\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import qai_hub as hub\n",
        "\n",
        "# You must configure your API token to authenticate with the AI Hub.\n",
        "# Replace \"<YOUR_API_TOKEN>\" with your actual token.\n",
        "# DO NOT share your token.\n",
        "print(\"2. Configuring Qualcomm AI Hub...\")\n",
        "api_token = \"nak7kyh0inngt9vewsxy74gobp4mk6q5zeean82x\" # <-- IMPORTANT: Replace with your API token\n",
        "\n",
        "!qai-hub configure --api_token {api_token}\n",
        "print(\"Configuration complete. Your API token is now set.\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Download and Prepare the SmolLM Model\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "\n",
        "# We'll use the SmolLM-135M-Instruct model from Hugging Face as an example.\n",
        "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
        "print(f\"3. Downloading and preparing model: {model_name}...\")\n",
        "\n",
        "# Load the tokenizer and the model from Hugging Face.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode. This is important for conversion.\n",
        "model.eval()\n",
        "\n",
        "# To handle the dynamic nature of the past_key_values cache, we will\n",
        "# create a simple wrapper class for the model's forward pass.\n",
        "# This ensures the output is a simple tuple of tensors, which ONNX can handle.\n",
        "class SmolLMWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = model.config\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # The model's forward pass returns a tuple of logits and past_key_values\n",
        "        outputs = self.model(input_ids, return_dict=True)\n",
        "\n",
        "        # The outputs contain a DynamicCache object, which the ONNX exporter\n",
        "        # cannot handle. We need to convert it into a flat list of tensors.\n",
        "        logits = outputs.logits\n",
        "        past_key_values = outputs.past_key_values\n",
        "\n",
        "        # Flatten the tuple of tuples of tensors into a single tuple of tensors.\n",
        "        flattened_past_key_values = []\n",
        "        for layer_key_value in past_key_values:\n",
        "            flattened_past_key_values.extend(layer_key_value)\n",
        "\n",
        "        # The ONNX exporter requires the output to be a tuple of tensors.\n",
        "        return (logits,) + tuple(flattened_past_key_values)\n",
        "\n",
        "# Instantiate the wrapper and get the output names\n",
        "model_wrapper = SmolLMWrapper(model)\n",
        "num_layers = model.config.num_hidden_layers\n",
        "output_names = ['logits'] + [f'past_key_values_{i}' for i in range(num_layers * 2)]\n",
        "\n",
        "# Define the sample prompt text in a single, consistent location.\n",
        "# This variable will be used for both ONNX export and inference.\n",
        "prompt_text = \"Who was Albert Einstein?\"\n",
        "\n",
        "# We will compile the model for a fixed input size to avoid dynamic shape issues.\n",
        "# A length of 50 tokens should be sufficient for a short response.\n",
        "max_input_length = 50\n",
        "input_shape = (1, max_input_length)\n",
        "dummy_input = torch.ones(input_shape, dtype=torch.int32)\n",
        "\n",
        "onnx_model_path = \"SmolLM-135M-Instruct.onnx\"\n",
        "print(f\"Exporting model to ONNX with a fixed input shape: {input_shape}...\")\n",
        "\n",
        "try:\n",
        "    # Use the wrapper model for export. We remove dynamic axes for this fixed-shape approach.\n",
        "    torch.onnx.export(\n",
        "        model_wrapper,\n",
        "        dummy_input,\n",
        "        onnx_model_path,\n",
        "        opset_version=14,  # Choose a compatible ONNX opset version\n",
        "        input_names=['input_ids'],\n",
        "        output_names=output_names,\n",
        "    )\n",
        "    print(f\"Model successfully exported to {onnx_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during ONNX export: {e}\")\n",
        "    onnx_model_path = None\n",
        "print(\"=\"*80)\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Compile the Model for QCS8550 NPU\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Define the target device\n",
        "#target_device = hub.Device(\"QCS8250 (Proxy)\")\n",
        "target_device = hub.Device(\"QCS8550 (Proxy)\")\n",
        "# Check if the device is available before submitting the job\n",
        "print(f\"4. Checking for device availability: {target_device.name}...\")\n",
        "available_devices = hub.get_devices()\n",
        "if target_device.name not in [d.name for d in available_devices]:\n",
        "    print(f\"ERROR: The device '{target_device.name}' is not currently available.\")\n",
        "    print(\"Please check the Qualcomm AI Hub website for device status and try again later.\")\n",
        "    compiled_model = None\n",
        "else:\n",
        "    print(f\"Device '{target_device.name}' is available. Submitting compilation job...\")\n",
        "    # Submit a compilation job to the Qualcomm AI Hub using the ONNX model file.\n",
        "    if onnx_model_path:\n",
        "        try:\n",
        "            # We are now targeting the QNN runtime for NPU execution.\n",
        "            compile_job = hub.submit_compile_job(\n",
        "                model=onnx_model_path,\n",
        "                name=f\"{model_name.split('/')[-1]}_qcs8550_npu\", # Updated name\n",
        "                device=target_device,\n",
        "                # The input specs now use the fixed input shape.\n",
        "                input_specs={\"input_ids\": (input_shape, \"int32\")},\n",
        "                options=\"--truncate_64bit_io\"\n",
        "            )\n",
        "\n",
        "            print(\"Compilation job submitted.\")\n",
        "            try:\n",
        "                print(f\"Job ID: {compile_job.id}\")\n",
        "            except AttributeError:\n",
        "                print(\"Could not retrieve job ID from the CompileJob object.\")\n",
        "                print(f\"You can check the job status and details on the AI Hub website using the URL: {compile_job.url}\")\n",
        "\n",
        "            # Wait for the job to complete. This is a blocking call and will\n",
        "            # raise an exception if the job fails.\n",
        "            print(\"Waiting for compilation to complete...\")\n",
        "            compile_job.wait()\n",
        "\n",
        "            # If wait() completes successfully, the model is ready.\n",
        "            print(\"Compilation job completed successfully!\")\n",
        "            compiled_model = compile_job.get_target_model()\n",
        "\n",
        "            # Handle the case where the Model object might not have an 'id' attribute.\n",
        "            try:\n",
        "                print(f\"Compiled model is ready. Model ID: {compiled_model.id}\")\n",
        "            except AttributeError:\n",
        "                print(\"Compiled model is ready, but could not retrieve the Model ID from the Model object.\")\n",
        "                print(\"Please check the Qualcomm AI Hub website to find the model and its ID.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during compilation: {e}\")\n",
        "            compiled_model = None\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Run Inference on the NPU using the Compiled Model and generate all tokens.\n",
        "# ------------------------------------------------------------------------------\n",
        "if compiled_model:\n",
        "    print(\"5. Submitting a single inference job for the initial prompt and generating all tokens...\")\n",
        "    try:\n",
        "        # The prompt_text variable is already defined and used for compilation.\n",
        "        print(f\"Initial Prompt: '{prompt_text}'\")\n",
        "\n",
        "        # The inference job expects a dictionary of inputs. We need to tokenize\n",
        "        # the prompt and pad it to the fixed length for a single inference pass.\n",
        "        # We also need an attention mask to handle the padded tokens correctly.\n",
        "        input_tokens = tokenizer(prompt_text, return_tensors=\"pt\", max_length=max_input_length, padding=\"max_length\", truncation=True).input_ids\n",
        "\n",
        "        # Check if the input shape matches the one the model was compiled with.\n",
        "        current_input_shape = input_tokens.shape\n",
        "        if current_input_shape != input_shape:\n",
        "            print(f\"ERROR: Input shape mismatch. Expected {input_shape} but got {current_input_shape}.\")\n",
        "            print(\"To fix this, either re-run the notebook from the start or ensure the prompt is shorter than the max_input_length.\")\n",
        "            raise ValueError(\"Input shape mismatch\")\n",
        "\n",
        "        # We explicitly cast the NumPy array to int32 to match the compiled model's\n",
        "        # expected data type, which is now int32.\n",
        "        input_data = {\"input_ids\": [input_tokens.cpu().numpy().astype(np.int32)]}\n",
        "\n",
        "        # Submit the inference job with the compiled model and input data.\n",
        "        inference_job = hub.submit_inference_job(\n",
        "            model=compiled_model,\n",
        "            device=target_device,\n",
        "            inputs=input_data\n",
        "        )\n",
        "\n",
        "        print(\"Inference job submitted.\")\n",
        "        try:\n",
        "            print(f\"Job ID: {inference_job.id}\")\n",
        "        except AttributeError:\n",
        "            print(\"Could not retrieve job ID from the InferenceJob object.\")\n",
        "            print(f\"You can check the job status and details on the AI Hub website using the URL: {inference_job.url}\")\n",
        "\n",
        "        # Wait for the inference job to complete.\n",
        "        print(\"Waiting for inference to complete...\")\n",
        "        inference_job.wait()\n",
        "\n",
        "        # If wait() completes successfully, the output is ready.\n",
        "        print(\"Inference job completed successfully!\")\n",
        "\n",
        "        # Get the output data, which will be a dictionary of numpy arrays.\n",
        "        output_data = inference_job.download_output_data()\n",
        "\n",
        "        if output_data is not None and 'output_0' in output_data:\n",
        "            # The output is a list containing a single NumPy array.\n",
        "            # The shape of the logits is (1, max_input_length, vocab_size).\n",
        "            logits = output_data['output_0'][0]\n",
        "\n",
        "            # We need to get the token IDs from the logits for each step.\n",
        "            predicted_token_ids = np.argmax(logits, axis=-1)\n",
        "\n",
        "            # The number of tokens in the original prompt.\n",
        "            initial_prompt_length = tokenizer(prompt_text).input_ids\n",
        "\n",
        "            # Extract only the newly generated tokens.\n",
        "            generated_token_ids = predicted_token_ids[0, len(initial_prompt_length):]\n",
        "\n",
        "            # Decode the generated tokens to get the final text.\n",
        "            generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"Full Generated Response:\")\n",
        "            print(prompt_text + generated_text)\n",
        "            print(\"=\"*80)\n",
        "        else:\n",
        "            print(\"Output data is None, or 'output_0' key is missing. Inference job may have failed.\")\n",
        "            if output_data is not None:\n",
        "                print(\"Here are all the keys found in the output data for debugging:\")\n",
        "                print(output_data.keys())\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during inference: {e}\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ]
}